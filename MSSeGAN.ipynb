{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KoUj7ek3gWOA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IsUagiCyx6Yq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/e0/be401c003291b56efc55aeba6a80ab790d3d4cece2778288d65323009420/pip-19.1.1-py2.py3-none-any.whl (1.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.4MB 18.8MB/s \n",
      "\u001b[31mmenpo 0.8.1 has requirement matplotlib<2.0,>=1.4, but you'll have matplotlib 3.0.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mmenpo 0.8.1 has requirement pillow<5.0,>=3.0, but you'll have pillow 5.4.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mmenpo 0.8.1 has requirement scipy<1.0,>=0.16, but you'll have scipy 1.2.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mfastai 1.0.42 has requirement torch>=1.0.0, but you'll have torch 0.4.1 which is incompatible.\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Found existing installation: pip 10.0.1\n",
      "    Uninstalling pip-10.0.1:\n",
      "      Successfully uninstalled pip-10.0.1\n",
      "Successfully installed pip-19.1.1\n",
      "Collecting torch\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/60/f685fb2cfb3088736bafbc9bdbb455327bdc8906b606da9c9a81bae1c81e/torch-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (676.9MB)\n",
      "\u001b[K     |████████████████████████████████| 676.9MB 67kB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/site-packages (from torch) (1.15.4)\n",
      "Installing collected packages: torch\n",
      "  Found existing installation: torch 0.4.1\n",
      "    Uninstalling torch-0.4.1:\n",
      "      Successfully uninstalled torch-0.4.1\n",
      "Successfully installed torch-1.1.0\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/site-packages (0.2.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/site-packages (from torchvision) (1.15.4)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/site-packages (from torchvision) (5.4.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/site-packages (from torchvision) (1.12.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/site-packages (from torchvision) (1.1.0)\n",
      "Collecting torchfusion\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/60/23d2485a906961097c48f9e4a457dbb217ade35370600f00e02f7fdcec3a/torchfusion-0.3.6-py3-none-any.whl (67kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 1.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: tensorboardX in /usr/local/lib/python3.6/site-packages (from torchfusion) (1.2)\n",
      "Collecting visdom (from torchfusion)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/97/c4/5f5356fd57ae3c269e0e31601ea6487e0622fedc6756a591e4a5fd66cc7a/visdom-0.1.8.8.tar.gz (1.4MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4MB 5.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/site-packages (from torchfusion) (1.15.4)\n",
      "Requirement already satisfied: torchtext in /usr/local/lib/python3.6/site-packages (from torchfusion) (0.3.1)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/site-packages (from torchfusion) (0.2.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/site-packages (from torchfusion) (4.30.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/site-packages (from torchfusion) (3.0.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/site-packages (from tensorboardX->torchfusion) (1.12.0)\n",
      "Requirement already satisfied: protobuf>=0.3.2 in /usr/local/lib/python3.6/site-packages (from tensorboardX->torchfusion) (3.6.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/site-packages (from visdom->torchfusion) (1.2.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/site-packages (from visdom->torchfusion) (2.21.0)\n",
      "Requirement already satisfied: tornado in /usr/local/lib/python3.6/site-packages (from visdom->torchfusion) (5.1.1)\n",
      "Requirement already satisfied: pyzmq in /usr/local/lib/python3.6/site-packages (from visdom->torchfusion) (17.1.2)\n",
      "Collecting torchfile (from visdom->torchfusion)\n",
      "  Downloading https://files.pythonhosted.org/packages/91/af/5b305f86f2d218091af657ddb53f984ecbd9518ca9fe8ef4103a007252c9/torchfile-0.1.0.tar.gz\n",
      "Collecting websocket-client (from visdom->torchfusion)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/19/44753eab1fdb50770ac69605527e8859468f3c0fd7dc5a76dd9c4dbd7906/websocket_client-0.56.0-py2.py3-none-any.whl (200kB)\n",
      "\u001b[K     |████████████████████████████████| 204kB 42.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.6/site-packages (from visdom->torchfusion) (5.4.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/site-packages (from torchtext->torchfusion) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/site-packages (from matplotlib->torchfusion) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/site-packages (from matplotlib->torchfusion) (2.7.5)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/site-packages (from matplotlib->torchfusion) (1.0.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/site-packages (from matplotlib->torchfusion) (2.3.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/site-packages (from protobuf>=0.3.2->tensorboardX->torchfusion) (40.6.3)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/site-packages (from requests->visdom->torchfusion) (2.8)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/site-packages (from requests->visdom->torchfusion) (1.22)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/site-packages (from requests->visdom->torchfusion) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/site-packages (from requests->visdom->torchfusion) (2018.11.29)\n",
      "Building wheels for collected packages: visdom, torchfile\n",
      "  Building wheel for visdom (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/ee/87/ce/a5023722374ca73b57fc8d4284ba6f973c01219b3c385a07e0\n",
      "  Building wheel for torchfile (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/b1/c3/d6/9a1cc8f3a99a0fc1124cae20153f36af59a6e683daca0a0814\n",
      "Successfully built visdom torchfile\n",
      "Installing collected packages: torchfile, websocket-client, visdom, torchfusion\n",
      "Successfully installed torchfile-0.1.0 torchfusion-0.3.6 visdom-0.1.8.8 websocket-client-0.56.0\n",
      "Requirement already satisfied: tensorboardx in /usr/local/lib/python3.6/site-packages (1.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/site-packages (from tensorboardx) (1.12.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/site-packages (from tensorboardx) (1.15.4)\n",
      "Requirement already satisfied: protobuf>=0.3.2 in /usr/local/lib/python3.6/site-packages (from tensorboardx) (3.6.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/site-packages (from protobuf>=0.3.2->tensorboardx) (40.6.3)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.6/site-packages (5.4.0)\n",
      "Collecting pydicom\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/88/d3c419ab2e753e7651510882a53219373e78fb55294cb247dffd3934ea55/pydicom-1.2.2-py2.py3-none-any.whl (7.0MB)\n",
      "\u001b[K     |████████████████████████████████| 7.0MB 1.6MB/s \n",
      "\u001b[?25hInstalling collected packages: pydicom\n",
      "Successfully installed pydicom-1.2.2\n",
      "Requirement already up-to-date: keras in /usr/local/lib/python3.6/site-packages (2.2.4)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /usr/local/lib/python3.6/site-packages (from keras) (1.15.4)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.14 in /usr/local/lib/python3.6/site-packages (from keras) (1.2.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.9.0 in /usr/local/lib/python3.6/site-packages (from keras) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.6/site-packages (from keras) (3.13)\n",
      "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/site-packages (from keras) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: keras_applications>=1.0.6 in /usr/local/lib/python3.6/site-packages (from keras) (1.0.7)\n",
      "Requirement already satisfied, skipping upgrade: keras_preprocessing>=1.0.5 in /usr/local/lib/python3.6/site-packages (from keras) (1.0.9)\n",
      "Collecting keras.utils\n",
      "  Downloading https://files.pythonhosted.org/packages/31/a2/8be2aee1c8cd388e83d447556c2c84a396944c8bad93d710c5e757f8e98e/keras-utils-1.0.13.tar.gz\n",
      "Requirement already satisfied, skipping upgrade: Keras>=2.1.5 in /usr/local/lib/python3.6/site-packages (from keras.utils) (2.2.4)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /usr/local/lib/python3.6/site-packages (from Keras>=2.1.5->keras.utils) (1.15.4)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.14 in /usr/local/lib/python3.6/site-packages (from Keras>=2.1.5->keras.utils) (1.2.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.9.0 in /usr/local/lib/python3.6/site-packages (from Keras>=2.1.5->keras.utils) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.6/site-packages (from Keras>=2.1.5->keras.utils) (3.13)\n",
      "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/site-packages (from Keras>=2.1.5->keras.utils) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: keras_applications>=1.0.6 in /usr/local/lib/python3.6/site-packages (from Keras>=2.1.5->keras.utils) (1.0.7)\n",
      "Requirement already satisfied, skipping upgrade: keras_preprocessing>=1.0.5 in /usr/local/lib/python3.6/site-packages (from Keras>=2.1.5->keras.utils) (1.0.9)\n",
      "Building wheels for collected packages: keras.utils\n",
      "  Building wheel for keras.utils (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/46/25/27/7707005c1cb27e1ffc8277b004ac295e34767b02b44d73d6be\n",
      "Successfully built keras.utils\n",
      "Installing collected packages: keras.utils\n",
      "Successfully installed keras.utils\n",
      "Collecting opencv-python\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/d2/a2dbf83d4553ca6b3701d91d75e42fe50aea97acdc00652dca515749fb5d/opencv_python-4.1.0.25-cp36-cp36m-manylinux1_x86_64.whl (26.6MB)\n",
      "\u001b[K     |████████████████████████████████| 26.6MB 1.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/site-packages (from opencv-python) (1.15.4)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.1.0.25\n",
      "Collecting nibabel\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/66/30/fbed62172920c3fd050b6483541546a87c5e735f4a0ef03f08bb150680b4/nibabel-2.4.1-py2.py3-none-any.whl (3.3MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3MB 1.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: six>=1.3 in /usr/local/lib/python3.6/site-packages (from nibabel) (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.8 in /usr/local/lib/python3.6/site-packages (from nibabel) (1.15.4)\n",
      "Installing collected packages: nibabel\n",
      "Successfully installed nibabel-2.4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%matplotlib inline\n",
    "%autoreload 2\n",
    "\n",
    "!pip3 install --upgrade pip\n",
    "!pip3 install --upgrade torch\n",
    "!pip3 install torchvision\n",
    "!pip3 install torchfusion\n",
    "!pip3 install tensorboardx\n",
    "!pip3 install pillow\n",
    "!pip3 install pydicom\n",
    "!pip3 install --upgrade keras\n",
    "!pip3 install --upgrade keras.utils\n",
    "!pip3 install opencv-python\n",
    "!pip3 install nibabel\n",
    "\n",
    "import os\n",
    "import io\n",
    "import errno\n",
    "import scipy\n",
    "import pydicom as dicom\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import random\n",
    "import nibabel as nib\n",
    "import datetime\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "from tensorflow import nn, layers\n",
    "from tensorflow.contrib import layers as clayers \n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.layers.core import Dropout, Lambda, Dense, Flatten\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.merge import Add, Concatenate\n",
    "from keras.layers import concatenate\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Input, Activation, Flatten, ZeroPadding2D, UpSampling2D, MaxPooling2D, merge, Convolution2D, Cropping2D\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.engine import InputSpec\n",
    "from keras.engine.topology import Layer\n",
    "from keras.utils import conv_utils\n",
    "from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "from keras import losses\n",
    "\n",
    "import torch\n",
    "import torch.cuda as cuda\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.distributions import Normal\n",
    "\n",
    "import torchvision.utils as vutils\n",
    "from torchvision import transforms, utils, datasets\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from IPython import display\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from getpass import getpass\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "import warnings\n",
    "warnings.filterwarnings(action = 'once')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1FaCEFNFx_-t"
   },
   "outputs": [],
   "source": [
    "class Logger:\n",
    "\n",
    "    def __init__(self, model_name, data_name):\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.data_name = data_name\n",
    "\n",
    "        self.comment = '{}_{}'.format(model_name, data_name)\n",
    "        self.data_subdir = '{}/{}'.format(model_name, data_name)\n",
    "\n",
    "        # TensorBoard\n",
    "        self.writer = SummaryWriter(comment=self.comment)\n",
    "\n",
    "    def log(self, d_error, g_error, nrmse, epoch, n_batch, num_batches):\n",
    "\n",
    "        if isinstance(d_error, torch.autograd.Variable):\n",
    "            d_error = d_error.data.cpu().numpy()\n",
    "        if isinstance(g_error, torch.autograd.Variable):\n",
    "            g_error = g_error.data.cpu().numpy()\n",
    "\n",
    "        step = Logger._step(epoch, n_batch, num_batches)\n",
    "        self.writer.add_scalar(self.comment + \"/D Error\", d_error, step)\n",
    "        self.writer.add_scalar(self.comment + \"/D(G(x)) Error\", g_error[0] + g_error[1], step)\n",
    "        self.writer.add_scalar(self.comment + \"/Dice\", nrmse, step)\n",
    "        \n",
    "    def add_images(self, images, nrows, direc, step, normalize):\n",
    "        \n",
    "        img_name = '{}/images{}'.format(self.comment, str(direc))\n",
    "\n",
    "        horizontal_grid = vutils.make_grid(\n",
    "            images, normalize=normalize, scale_each=True)\n",
    "\n",
    "        self.writer.add_image(img_name, horizontal_grid, step)\n",
    "        \n",
    "    def log_images(self, generated, real, sampled, num_images, epoch, n_batch, num_batches, format='NCHW', normalize=True):\n",
    "        if type(generated) == np.ndarray:\n",
    "            generated = torch.from_numpy(generated)\n",
    "            \n",
    "        if type(real) == np.ndarray:\n",
    "            real = torch.from_numpy(real)\n",
    "            \n",
    "        if type(sampled) == np.ndarray:\n",
    "            sampled = torch.from_numpy(sampled)\n",
    "            \n",
    "        if format=='NHWC':\n",
    "            generated = generated.transpose(1,3)\n",
    "            real = real.transpose(1,3)\n",
    "            sampled = sampled.transpose(1, 3)\n",
    "\n",
    "        step = Logger._step(epoch, n_batch, num_batches)\n",
    "        nrows = int(np.sqrt(num_images))\n",
    "\n",
    "        self.add_images(generated, nrows, '/generated_mask', step, normalize)\n",
    "        self.add_images(sampled, nrows, '/original_mri', step, normalize)\n",
    "        self.add_images(real, nrows, '/ground_truth_mask', step, normalize)\n",
    "        \n",
    "\n",
    "    def save_torch_images(self, horizontal_grid, grid, epoch, n_batch, plot_horizontal=True):\n",
    "        out_dir = './data/images/{}'.format(self.data_subdir)\n",
    "        Logger._make_dir(out_dir)\n",
    "\n",
    "        # Plot and save horizontal\n",
    "        fig = plt.figure(figsize=(128, 128))\n",
    "        plt.imshow(np.moveaxis(horizontal_grid.numpy(), 0, -1))\n",
    "        plt.axis('off')\n",
    "        if plot_horizontal:\n",
    "            display.display(plt.gcf())\n",
    "        self._save_images(fig, epoch, n_batch, 'hori')\n",
    "        plt.close()\n",
    "\n",
    "        # Save squared\n",
    "        fig = plt.figure()\n",
    "        plt.imshow(np.moveaxis(grid.numpy(), 0, -1))\n",
    "        plt.axis('off')\n",
    "        self._save_images(fig, epoch, n_batch)\n",
    "        plt.close()\n",
    "\n",
    "    def _save_images(self, fig, epoch, n_batch, comment=''):\n",
    "        out_dir = './data/images/{}'.format(self.data_subdir)\n",
    "        Logger._make_dir(out_dir)\n",
    "        fig.savefig('{}/{}_epoch_{}_batch_{}.png'.format(out_dir,\n",
    "                                                         comment, epoch, n_batch))\n",
    "\n",
    "    def display_status(self, epoch, num_epochs, n_batch, num_batches, d_error, g_error, d_pred_real, d_pred_fake, \n",
    "                       d_acc_real, d_acc_fake):\n",
    "        \n",
    "        # var_class = torch.autograd.variable.Variable\n",
    "        if isinstance(d_error, torch.autograd.Variable):\n",
    "            d_error = d_error.data.cpu().numpy()\n",
    "        if isinstance(g_error, torch.autograd.Variable):\n",
    "            g_error = g_error.data.cpu().numpy()\n",
    "        if isinstance(d_pred_real, torch.autograd.Variable):\n",
    "            d_pred_real = d_pred_real.data\n",
    "        if isinstance(d_pred_fake, torch.autograd.Variable):\n",
    "            d_pred_fake = d_pred_fake.data\n",
    "        \n",
    "        clear_output()\n",
    "        print('Epoch: [{}/{}], Batch Num: [{}/{}]'.format(\n",
    "            epoch,num_epochs, n_batch, num_batches)\n",
    "             )\n",
    "        print('Discriminator Loss: {:.4f}'.format(d_error))\n",
    "        print('    Generator Loss: {:.4f}'.format(g_error[0] + g_error[1]))\n",
    "        #print('Discriminator Loss: {:.4f}, Generator Loss: {:.4f}'.format(d_error, g_error))\n",
    "        print('D(x) Loss: {:.4f}, D(G(x)) Loss: {:.4f}'.format(d_pred_real.mean(), d_pred_fake.mean()))\n",
    "        print('D(x) Acc:  {:.4f}, D(G(x)) Acc:  {:.4f}'.format(d_acc_real, d_acc_fake))\n",
    "\n",
    "    def save_models(self, generator, discriminator, epoch):\n",
    "        out_dir = './data/models/{}'.format(self.data_subdir)\n",
    "        Logger._make_dir(out_dir)\n",
    "        torch.save(generator.state_dict(),\n",
    "                   '{}/G_epoch_{}'.format(out_dir, epoch))\n",
    "        torch.save(discriminator.state_dict(),\n",
    "                   '{}/D_epoch_{}'.format(out_dir, epoch))\n",
    "\n",
    "    def close(self):\n",
    "        self.writer.close()\n",
    "\n",
    "    # Private Functionality\n",
    "\n",
    "    @staticmethod\n",
    "    def _step(epoch, n_batch, num_batches):\n",
    "        return epoch * num_batches + n_batch\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_dir(directory):\n",
    "        try:\n",
    "            os.makedirs(directory)\n",
    "        except OSError as e:\n",
    "            if e.errno != errno.EEXIST:\n",
    "                raise\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eiG8ZVcOFM36"
   },
   "outputs": [],
   "source": [
    "import warnings;\n",
    "warnings.filterwarnings('ignore');\n",
    "\n",
    "class MSDataset(Dataset): \n",
    "          \n",
    "    def __init__(self, transforms=None):\n",
    "        \n",
    "        self.transforms = transforms\n",
    "        \n",
    "        direc = \"/floyd/home\"\n",
    "        \n",
    "        os.chdir(direc)\n",
    "        user = getpass('BitBucket user')\n",
    "        password = getpass('BitBucket password')\n",
    "        os.environ['BITBUCKET_AUTH'] = user + ':' + password.replace(\"@\", \"%40\")\n",
    "\n",
    "        !git clone https://$BITBUCKET_AUTH@bitbucket.org/sidijju/msdata.git\n",
    "        \n",
    "        root = direc + \"/msdata/\"\n",
    "        \n",
    "        self.mri = []\n",
    "        self.mask = []\n",
    "        theta = (np.arange(-15, 15, 15) * np.pi) / 180.\n",
    "        \n",
    "        good_indices = []\n",
    "        \n",
    "        print(\"Started file processing\")\n",
    "        \n",
    "        index = 0\n",
    "        for filename in sorted(os.listdir(root)):\n",
    "            print(filename)\n",
    "            if \"FLAIR\" in filename:\n",
    "                img = nib.load(root + filename)\n",
    "                dat = img.get_data()\n",
    "                dat = np.rollaxis(dat, 1)\n",
    "                for image in dat:\n",
    "                    image = image.astype(float)\n",
    "                    image = np.expand_dims(preprocessing.normalize(image), axis = -1)\n",
    "                    image = image[:, 150:480, :]\n",
    "                    self.mri.append(image)\n",
    "                    \n",
    "            if \"mask\" in filename:\n",
    "                img = nib.load(root + filename)\n",
    "                dat = img.get_data()\n",
    "                dat = np.rollaxis(dat, 1)\n",
    "                for image in dat:\n",
    "                    image = image.astype(float)\n",
    "                    image = np.expand_dims(image, axis = -1)\n",
    "                    image = image[:, 150:480, :]\n",
    "                    if not (np.amax(image) == 0.0):\n",
    "                        good_indices.append(index)\n",
    "                    self.mask.append(image)\n",
    "                    index += 1\n",
    "        \n",
    "        print(\"Completed file processing\")\n",
    "        \n",
    "        self.mri_final = []\n",
    "        self.mask_final = []\n",
    "        \n",
    "        for j in range(1):\n",
    "            for i in good_indices:\n",
    "                self.mri_final.append(self.mri[i])\n",
    "                self.mask_final.append(self.mask[i])\n",
    "\n",
    "        self.mri = np.dstack(self.mri_final)\n",
    "        self.mask = np.dstack(self.mask_final)\n",
    "        \n",
    "        self.data = np.stack((self.mri, self.mask), axis=3)\n",
    "        \n",
    "        self.data = np.moveaxis(self.data, 0, 2)\n",
    "        self.data = np.moveaxis(self.data, 0, 1)\n",
    "        \n",
    "        print(\"Completed\")\n",
    "        \n",
    "        print(self.data.shape)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DfKDJLeoyH42"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "BitBucket user \n",
      "BitBucket password \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'msdata' already exists and is not an empty directory.\n",
      "Started file processing\n",
      ".git\n",
      "patient26_FLAIR.nii.gz\n",
      "patient26_mask.nii.gz\n",
      "patient27_FLAIR.nii.gz\n",
      "patient27_mask.nii.gz\n",
      "patient28_FLAIR.nii.gz\n",
      "patient28_mask.nii.gz\n",
      "patient29_FLAIR.nii.gz\n",
      "patient29_mask.nii.gz\n",
      "patient30_FLAIR.nii.gz\n",
      "patient30_mask.nii.gz\n",
      "Completed file processing\n",
      "Completed\n",
      "(1021, 330, 192, 2)\n"
     ]
    }
   ],
   "source": [
    "r_dim = 330\n",
    "c_dim = 192\n",
    "\n",
    "image_shape = (r_dim, c_dim, 1)\n",
    "\n",
    "dataset = MSDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vwsqPl3S1Is8"
   },
   "source": [
    "## Define Different Types of Blocks\n",
    "\n",
    "---\n",
    "We will use two types of blocks in this GAN. \n",
    "\n",
    "### Resnet Block\n",
    "\n",
    "\n",
    "---\n",
    "The Resnet block consists of two convolution and activation layers, along with a direct connection between the input and the output layer. This serves to allow more information from the original image to flow through in the neural network.\n",
    "\n",
    "![Diagram of Resnet Block](https://cdn-images-1.medium.com/max/1200/1*ByrVJspW-TefwlH7OLxNkg.png)\n",
    "\n",
    "### Unet\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "The Unet consists of two different segments, the encoder and decoder segments. The difference between a standard encoder decoder and a Unet is that Unet also employs residual connections similar to the Resnet block. While generally used for segmentation based tasks, the Unet will be used in this task to determine areas of the image that are incorrect and thus de-alias the image. \n",
    "\n",
    "![Diagram of Unet](https://gluon.mxnet.io/_images/Pixel2pixel-Unet.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CtXlJcDqyIas"
   },
   "outputs": [],
   "source": [
    "def convolution_block(x, filters, kernel_size, strides=(1,1), padding='same', activation=True):\n",
    "    x = Conv2D(filters, kernel_size=kernel_size, strides=strides, padding=padding)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    if activation == True:\n",
    "        x = Activation(\"relu\")(x)\n",
    "    return x\n",
    "\n",
    "def res_block(input, filters, kernel_size = (3,3), strides=(1,1), use_dropout=False):\n",
    "    x = Activation(\"relu\")(input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = convolution_block(x, filters, kernel_size )\n",
    "    x = convolution_block(x, filters, kernel_size, activation=False)\n",
    "    x = Add()([x, input])\n",
    "    return x\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CmrZieIG9SS9"
   },
   "source": [
    "## Define the Generator and the Discriminator\n",
    "\n",
    "---\n",
    "\n",
    "### Generator\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "The Generator model will take in an input of an aliased MRI image and output a de-aliased one. Its goal is to increase its chances of fooling the discriminator into thinking that it's producing the actual real image. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ez9vFFf-9U4o"
   },
   "outputs": [],
   "source": [
    "#Number of filters\n",
    "ngf = 64\n",
    "\n",
    "#Input and Output number of channels\n",
    "input_nc = 1\n",
    "output_nc = 1\n",
    "\n",
    "#Input Shape of the Generator\n",
    "input_shape_generator = (330, 192, input_nc)\n",
    "\n",
    "def generator_model():\n",
    "    \n",
    "    concat_axis = -1\n",
    "\n",
    "    inputs = Input(shape=input_shape_generator)\n",
    "                          \n",
    "    conv1 = Conv2D(64, 5, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
    "    conv1 = Conv2D(64, 5, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = Conv2D(96, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "    conv2 = Conv2D(96, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    conv3 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "    conv3 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    conv4 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "    conv4 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    conv5 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "    conv5 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "\n",
    "    up_conv5 = UpSampling2D(size=(2, 2))(conv5)\n",
    "    ch, cw = get_crop_shape(conv4, up_conv5)\n",
    "    crop_conv4 = Cropping2D(cropping=(ch,cw))(conv4)\n",
    "    up6 = concatenate([up_conv5, crop_conv4], axis=concat_axis)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n",
    "\n",
    "    up_conv6 = UpSampling2D(size=(2, 2))(conv6)\n",
    "    ch, cw = get_crop_shape(conv3, up_conv6)\n",
    "    crop_conv3 = Cropping2D(cropping=(ch,cw))(conv3)\n",
    "    up7 = concatenate([up_conv6, crop_conv3], axis=concat_axis) \n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n",
    "\n",
    "    up_conv7 = UpSampling2D(size=(2, 2))(conv7)\n",
    "    ch, cw = get_crop_shape(conv2, up_conv7)\n",
    "    crop_conv2 = Cropping2D(cropping=(ch,cw))(conv2)\n",
    "    up8 = concatenate([up_conv7, crop_conv2], axis=concat_axis)\n",
    "    conv8 = Conv2D(96, (3, 3), activation='relu', padding='same')(up8)\n",
    "    conv8 = Conv2D(96, (3, 3), activation='relu', padding='same')(conv8)\n",
    "\n",
    "    up_conv8 = UpSampling2D(size=(2, 2))(conv8)\n",
    "    ch, cw = get_crop_shape(conv1, up_conv8)\n",
    "    crop_conv1 = Cropping2D(cropping=(ch,cw))(conv1)\n",
    "    up9 = concatenate([up_conv8, crop_conv1], axis=concat_axis)\n",
    "    conv9 = Conv2D(64, (3, 3), activation='relu', padding='same')(up9)\n",
    "    conv9 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv9)\n",
    "\n",
    "    ch, cw = get_crop_shape(inputs, conv9)\n",
    "    conv9 = ZeroPadding2D(padding=((ch[0], ch[1]), (cw[0], cw[1])))(conv9)\n",
    "    conv10 = Conv2D(1, (1, 1), activation = 'softmax')(conv9)\n",
    "\n",
    "    model = Model(input = inputs, output = conv10)\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_crop_shape(target, refer):\n",
    "        # width, the 3rd dimension\n",
    "        cw = (target.get_shape()[2] - refer.get_shape()[2]).value\n",
    "        assert (cw >= 0)\n",
    "        if cw % 2 != 0:\n",
    "            cw1, cw2 = int(cw/2), int(cw/2) + 1\n",
    "        else:\n",
    "            cw1, cw2 = int(cw/2), int(cw/2)\n",
    "        # height, the 2nd dimension\n",
    "        ch = (target.get_shape()[1] - refer.get_shape()[1]).value\n",
    "        assert (ch >= 0)\n",
    "        if ch % 2 != 0:\n",
    "            ch1, ch2 = int(ch/2), int(ch/2) + 1\n",
    "        else:\n",
    "            ch1, ch2 = int(ch/2), int(ch/2)\n",
    "\n",
    "        return (ch1, ch2), (cw1, cw2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hcMifMGJCi-n"
   },
   "source": [
    "### Discriminator\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "The Discriminator determines whether the output of the Generator is a real MRI image or whether it is \"fake\". Its goal is to be able to tell apart generated images from real ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2vCNaOY1CxNi"
   },
   "outputs": [],
   "source": [
    "#Number of filters\n",
    "ndf = 64\n",
    "\n",
    "#Number of output channels\n",
    "output_nc = 1\n",
    "\n",
    "#Input Shape of Image to the Discriminator\n",
    "input_shape_discriminator = (330, 192, output_nc)\n",
    "\n",
    "\n",
    "def discriminator_model():\n",
    "    n_layers, use_sigmoid = 3, True\n",
    "    inputs = Input(shape=input_shape_discriminator)\n",
    "\n",
    "    x = Conv2D(filters=ndf, kernel_size=(4,4), strides=2, padding='same')(inputs)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    nf_mult, nf_mult_prev = 1, 1\n",
    "    for n in range(n_layers):\n",
    "        nf_mult_prev, nf_mult = nf_mult, min(2**n, 8)\n",
    "        x = Conv2D(filters=ndf*nf_mult, kernel_size=(4,4), strides=2, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(0.2)(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "\n",
    "    nf_mult_prev, nf_mult = nf_mult, min(2**n_layers, 8)\n",
    "    x = Conv2D(filters=ndf*nf_mult, kernel_size=(4,4), strides=1, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    x = Conv2D(filters=1, kernel_size=(4,4), strides=1, padding='same')(x)\n",
    "    if use_sigmoid:\n",
    "        x = Activation('sigmoid')(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1024, activation='tanh')(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=x, name='Discriminator')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nSQnehykDbm2"
   },
   "source": [
    "## Putting It all Together\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "In this segment we combine the generator and discriminator models and define losses for each of them. Since this model has to have the generated images be as close to the real ones as possible, we have to have the outputs of the generated images in the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FQKEcy2oDbGw"
   },
   "outputs": [],
   "source": [
    "def gendis_mult_out(generator, discriminator):\n",
    "    inputs = Input(shape=(330, 192, 1))\n",
    "    generated_images = generator(inputs)\n",
    "    outputs = discriminator(generated_images)\n",
    "    model = Model(inputs=inputs, outputs=[generated_images, outputs])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iJxUv_7HEFO9"
   },
   "source": [
    "### Losses\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "We will use multiple losses in the model. \n",
    "#### Wasserstein Loss\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Wasserstein Loss is similar to Mean Squared Error in that it finds the mean of the differences between two images. It is known to improve GAN performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "txbSpy86EVWH"
   },
   "outputs": [],
   "source": [
    "def w_loss(y_true, y_pred):\n",
    "    return K.mean(y_true - y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ub9enQ66x5YD"
   },
   "source": [
    "#### Perceptual Loss\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Perceptual Loss uses the classic VGG16 architecture to determine similarity between images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fmzTMzPByCpS"
   },
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "def perceptual_loss(y_true, y_pred):\n",
    "    vgg = VGG16(include_top=False, weights='imagenet', input_shape=(g_res, g_res, 3))\n",
    "    loss_model = Model(inputs=vgg.input, outputs=vgg.get_layer('block3_conv3').output)\n",
    "    loss_model.trainable = False\n",
    "    y_true = K.concatenate((y_true, y_true, y_true), axis = 3)\n",
    "    y_pred = K.concatenate((y_pred, y_pred, y_pred), axis = 3)\n",
    "    loss_model_true = loss_model(y_true)\n",
    "    loss_model_pred = loss_model(y_pred)\n",
    "    mean = K.mean(K.square(loss_model_true - loss_model_pred))\n",
    "    return K.mean(K.square(loss_model(y_true) - loss_model(y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dYVKC01ZPeO3"
   },
   "source": [
    "#### Mean Squared Error\n",
    "\n",
    "___\n",
    "\n",
    "MSE calculates the pixel by pixel loss from the true output to the fake one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Prlp-JSz_a4I"
   },
   "outputs": [],
   "source": [
    "def mse(y_true, y_pred):\n",
    "    total = np.sum(np.square(np.subtract(y_true, y_pred)))/(g_res ** 2)\n",
    "    #print(\"mse: \", total)\n",
    "    return total\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WP0qs4ERFM4f"
   },
   "outputs": [],
   "source": [
    "def fft_mse(y_true, y_pred):\n",
    "    y_true_fft = tf.spectral.fft3d(tf.cast(y_true, tf.complex64))\n",
    "    y_pred_fft = tf.spectral.fft3d(tf.cast(y_pred, tf.complex64))\n",
    "    total = mse(tf.cast(y_true_fft, tf.float32), \n",
    "                tf.cast(y_pred_fft, tf.float32))\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth = 1.\n",
    "def dice(y_true, y_pred):\n",
    "    positives = K.greater_equal(y_pred, 0.5)\n",
    "    positives = K.cast(positives, K.floatx())\n",
    "    outputs = positives + ((1-positives)*y_pred)\n",
    "    y_true_f = K.abs(K.flatten(y_true))\n",
    "    y_pred_f = K.abs(K.flatten(outputs))\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1.-dice(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "urHpFZ9QjdJf"
   },
   "outputs": [],
   "source": [
    "def g_loss(y_true, y_pred):\n",
    "    dice_loss = dice_coef_loss(y_true, y_pred)\n",
    "    return dice_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WVvz_zmfEmh4"
   },
   "source": [
    "## Training the Model\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "We have now arrived at the final step of the GAN: defining the dataset and training the model. \n",
    "\n",
    "### Dataset\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "We will load in the OASIS Dataset using the custom class defined above. Since OASIS data is not public, I used a Bitbucket login to access the data and pull it into this notebook. This notebook will not work without that data. \n",
    "\n",
    "We will also define a method of removing information from the images in order to get aliased MRI images. This data will be x_train. \n",
    "\n",
    "The real images themselves will be y_train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cHzEkuU9FKc0"
   },
   "source": [
    "### Models and Optimizers\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Here we will initialize the Generator and Discriminator models and also define the optimizers for each. We will also define functions for metrics that the model will keep track of, but NOT train on over time (SSIM, NMSE, PSNR). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PSNR\n",
    "\n",
    "PSNR or Peak Signal-to-Noise Ratio is a measure of the noise introduced by error between two given images. The closer PSNR is to 100, the better quality the reconstruction is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def psnr(y_true, y_pred):\n",
    "    mse_val = mse(y_true, y_pred)\n",
    "    if mse_val == 0:\n",
    "        return 100\n",
    "    return math.log10(255.0/mse_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NRMSE\n",
    "\n",
    "NRMSE or Normalized Root MSE is a version of MSE that takes into account the images it compares. The closer NRMSE is to 0, the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.measure import compare_nrmse as nrmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SSIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.measure import compare_ssim as ssim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "inTsoO98FW41"
   },
   "source": [
    "### Compiling and Training\n",
    "\n",
    "---\n",
    "\n",
    "In this last step, we will compile our models with the losses we defined and train them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZviusrUxFq3G"
   },
   "source": [
    "Now we will process and define the batches of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nFvPdSiFLXtp"
   },
   "outputs": [],
   "source": [
    "currEpoch = 0\n",
    "\n",
    "EPOCH_NUM = 10\n",
    "\n",
    "BATCH_SIZE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "def loss_func(y_true, y_pred):\n",
    "    return dice_coef_loss(y_true, y_pred)\n",
    "\n",
    "g = load_model('best_7919.h5', custom_objects={'loss_func': loss_func,'dice': dice, \n",
    "                                                       'dice_coef_loss': dice_coef_loss})\n",
    "d = discriminator_model()\n",
    "d_on_g = gendis_mult_out(g, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_all_weights(d, g, epoch_number, current_loss):\n",
    "    now = datetime.datetime.now()\n",
    "    os.chdir(\"/floyd/home/\")\n",
    "    save_dir = os.path.join(\"weights/\", '{}_{}_{} run'.format(now.month, now.day, now.year))\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    g.save_weights(os.path.join(save_dir, 'generator_{}.h5'.format(epoch_number + currEpoch)), True)\n",
    "    d.save_weights(os.path.join(save_dir, 'discriminator_{}.h5'.format(epoch_number + currEpoch)), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hkWl-TnMFM5F"
   },
   "outputs": [],
   "source": [
    "# Initialize optimizers\n",
    "g_opt = Adam(epsilon=1E-4)\n",
    "d_opt = Adam(lr=1E-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "d_on_g_opt = Adam(lr=1E-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "# Compile models\n",
    "d.trainable = True\n",
    "d.compile(optimizer=d_opt, loss=w_loss)\n",
    "d.trainable = False\n",
    "\n",
    "loss = [g_loss, w_loss]\n",
    "loss_weights = [100, 1]\n",
    "d_on_g.compile(optimizer=d_on_g_opt, loss=loss, loss_weights=loss_weights)\n",
    "d.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_true_batch, output_false_batch = np.random.uniform(low = 0.9, high = 1.0, size = (BATCH_SIZE, 1)), np.zeros((BATCH_SIZE, 1))\n",
    "\n",
    "logger = Logger(model_name='MSSeGAN_v1', data_name='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y0AXkm2PFM5J"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(dataset.data[:, :, :, 0], dataset.data[:, :, :, 1], test_size=0.20)\n",
    "\n",
    "x_train = np.expand_dims(x_train, axis = -1)\n",
    "y_train = np.expand_dims(y_train, axis = -1)\n",
    "x_test = np.expand_dims(x_test, axis = -1)\n",
    "y_test = np.expand_dims(y_test, axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Js6cKZFFM5Q"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm.tqdm(range(EPOCH_NUM)):\n",
    "    \n",
    "    num_samples = int(x_train.shape[0])\n",
    "    \n",
    "    permutated_indexes = np.random.permutation(num_samples)\n",
    "    num_batches = int(num_samples / BATCH_SIZE)\n",
    "    \n",
    "    d_losses = []\n",
    "    d_on_g_losses = []\n",
    "    \n",
    "    for n_batch in range(num_batches):\n",
    "        batch_indexes = permutated_indexes[n_batch*BATCH_SIZE:(n_batch+1)*BATCH_SIZE]\n",
    "\n",
    "        sampled_batch = x_train[batch_indexes, :, :, :]\n",
    "        full_batch = y_train[batch_indexes, :, :, :]\n",
    "\n",
    "        generated_images = g.predict(x=sampled_batch, batch_size=BATCH_SIZE)\n",
    "\n",
    "        d_loss_real = d.train_on_batch(full_batch, output_true_batch)\n",
    "        d_loss_fake = d.train_on_batch(generated_images, output_false_batch)\n",
    "        d_loss = 0.5 * np.add(d_loss_fake, d_loss_real)\n",
    "        d_losses.append(d_loss)\n",
    "\n",
    "        d.trainable = False\n",
    "\n",
    "        d_on_g_loss = d_on_g.train_on_batch(sampled_batch, [full_batch, output_true_batch])\n",
    "        d_on_g_losses.append(d_on_g_loss)\n",
    "\n",
    "        d.trainable = True\n",
    "        \n",
    "        dice_val = 0\n",
    "        for i in range(BATCH_SIZE):\n",
    "            dice_val += K.eval(dice(K.cast(full_batch[i][:, :, 0], 'float32'), generated_images[i][:, :, 0]))\n",
    "\n",
    "        dice_val /= BATCH_SIZE\n",
    "        \n",
    "        logger.log(d_loss, d_on_g_loss, dice_val, epoch, n_batch, num_batches) \n",
    "        logger.log_images(generated_images[0:3], full_batch[0:3], sampled_batch[0:3], 3, epoch, n_batch, num_batches, format='NHWC')   \n",
    "    \n",
    "    with open('log.txt', 'a+') as f:\n",
    "        f.write('{} - {} - {}\\n'.format(epoch, np.mean(d_losses), np.mean(d_on_g_losses)))\n",
    "\n",
    "    save_all_weights(d, g, epoch, int(np.mean(d_on_g_losses)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MRIGAN_Unet_Final.ipynb",
   "private_outputs": true,
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
